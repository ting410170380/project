{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from flask import Flask\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "params=urllib.parse.quote_plus('Driver={ODBC Driver 17 for SQL Server};Server=DESKTOP-4JUNPCV;Database=News;Trusted_Connection=yes;')\n",
    "\n",
    "app=Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI']=f'mssql+pyodbc:///?odbc_connect={params}'\n",
    "db=SQLAlchemy(app)\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__='stories'\n",
    "    id=db.Column(db.INTEGER,primary_key=True)\n",
    "    category = db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    subcategory = db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    title=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    date=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    abstract=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    contents=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    url=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self,category,subcategory,title,date,abstract,contents,url):\n",
    "        self.category=category\n",
    "        self.subcategory=subcategory\n",
    "        self.title=title\n",
    "        self.date=date\n",
    "        self.abstract=abstract\n",
    "        self.contents=contents\n",
    "        self.url=url\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__='videos'\n",
    "    id=db.Column(db.INTEGER,primary_key=True)\n",
    "    title=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    date=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    abstract=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    contents=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    url=db.Column(db.String('max'),unique=False,nullable=False)\n",
    "    \n",
    "    \n",
    "    def __init__(self,title,date,abstract,contents,url):\n",
    "        self.title=title\n",
    "        self.date=date\n",
    "        self.abstract=abstract\n",
    "        self.contents=contents\n",
    "        self.url=url\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with app.app_context():\n",
    "        db.drop_all()\n",
    "        db.create_all()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章類\n",
    "import pyodbc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "conn=pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};''Server=DESKTOP-4JUNPCV;''Database=News;''Trusted_Connection=yes;')\n",
    "chrome_path = 'C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe'  \n",
    "service = Service(chrome_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = 'https://edition.cnn.com/'\n",
    "driver.get(url)\n",
    "cur = conn.cursor()\n",
    "cur.fast_executemany = False\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element(By.ID, 'headerSearchIcon').click() \n",
    "\n",
    "keyword = 'Artificial Intelligence'\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__input').send_keys(keyword)\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__submit').click() \n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[2]/div/div/ul/li[2]/label').click() \n",
    "time.sleep(10)\n",
    "\n",
    "#第一頁\n",
    "for x in range(1,10):                 \n",
    "    titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[1]/span').text  #標題\n",
    "    publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[2]').text   #日期  \n",
    "    abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "    elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]')\n",
    "    urls = elements.get_attribute('href')   #連結\n",
    "    \n",
    "    driver.get(urls)    \n",
    "    try:\n",
    "        categories = driver.find_element(By.XPATH, '/html/body/div[3]/section[3]/div[1]/a[1]').text  #類別\n",
    "        subcategories = driver.find_element(By.XPATH, '/html/body/div[3]/section[3]/div[1]/a[2]').text  #子類別\n",
    "    except:\n",
    "        categories = \" \"\n",
    "        subcategories = \" \"\n",
    "\n",
    "    try:\n",
    "            a = driver.find_element(By.XPATH, '/html/body/div[3]/section[4]/section[1]/section[1]/article/section/main/div[2]/div[1]')\n",
    "            time.sleep(2)\n",
    "            paragraphs = a.find_elements(By.TAG_NAME, 'p')\n",
    "            time.sleep(2)\n",
    "            content_str = [p.text for p in paragraphs]\n",
    "            content = \"\\n\".join(content_str)\n",
    "    except:\n",
    "        content = \" \"\n",
    "\n",
    "    cur.execute('''INSERT INTO stories(category,subcategory,title,date,abstract,contents,url) VALUES(?,?,?,?,?,?,?);''',(categories,subcategories,titles,publication_dates,abstracts,content,urls))\n",
    "    conn.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "\n",
    "for y in range(1,7):               \n",
    "    pages=driver.find_element(By.XPATH,'//*[@id=\"search\"]/div[2]/div/div[4]/div/div[3]').click()\n",
    "    time.sleep(10) \n",
    "    for z in range(1,10):                     \n",
    "        titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[1]/span').text  #標題\n",
    "        publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[2]').text   #日期  \n",
    "        abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "        elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]')\n",
    "        urls = elements.get_attribute('href')   #連結\n",
    "        \n",
    "        driver.get(urls)\n",
    "        try:\n",
    "            categories = driver.find_element(By.XPATH, '/html/body/div[3]/section[3]/div[1]/a[1]').text  #類別\n",
    "            subcategories = driver.find_element(By.XPATH, '/html/body/div[3]/section[3]/div[1]/a[2]').text  #子類別\n",
    "        except:\n",
    "            categories = \" \"\n",
    "            subcategories = \" \"\n",
    "\n",
    "        try:\n",
    "            a = driver.find_element(By.XPATH, '/html/body/div[3]/section[4]/section[1]/section[1]/article/section/main/div[2]/div[1]')\n",
    "            time.sleep(2)\n",
    "            paragraphs = a.find_elements(By.TAG_NAME, 'p')\n",
    "            time.sleep(2)\n",
    "            content_str = [p.text for p in paragraphs]\n",
    "            content = \"\\n\".join(content_str)\n",
    "        except:\n",
    "            content = \" \"\n",
    "\n",
    "        cur.execute('''INSERT INTO stories(category,subcategory,title,date,abstract,contents,url) VALUES(?,?,?,?,?,?,?);''',(categories,subcategories,titles,publication_dates,abstracts,content,urls))\n",
    "        conn.commit()\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#影片類\n",
    "import pyodbc\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "conn=pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};''Server=DESKTOP-4JUNPCV;''Database=News;''Trusted_Connection=yes;')\n",
    "chrome_path = 'C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe'  \n",
    "service = Service(chrome_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = 'https://edition.cnn.com/'\n",
    "driver.get(url)\n",
    "cur = conn.cursor()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element(By.ID, 'headerSearchIcon').click() \n",
    "\n",
    "keyword = 'Artificial Intelligence'\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__input').send_keys(keyword)\n",
    "driver.find_element(By.CLASS_NAME, 'search-bar__submit').click() \n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[2]/div/div/ul/li[3]/label').click() \n",
    "time.sleep(10)\n",
    "\n",
    "#第一頁\n",
    "for x in range(1,7):                 \n",
    "    titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[1]/span').text  #標題\n",
    "    publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[2]').text   #日期  \n",
    "    abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "    elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(x)+']/a[2]')\n",
    "    urls = elements.get_attribute('href')   #連結\n",
    "    \n",
    "    driver.get(urls)           \n",
    "    a = driver.find_elements(By.XPATH, '/html/body/div[1]/section[4]/section/div/section/div/div/div[1]/div/div[3]/div[5]')\n",
    "    time.sleep(2)\n",
    "    for item in a:\n",
    "        content = item.text   #內文\n",
    "    cur.execute('''INSERT INTO videos(title,date,abstract,contents,url) VALUES(?,?,?,?,?);''',(titles,publication_dates,abstracts,content,urls))\n",
    "    conn.commit()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(1)\n",
    "    \n",
    "for y in range(1,9):               \n",
    "    pages=driver.find_element(By.XPATH,'//*[@id=\"search\"]/div[2]/div/div[4]/div/div[3]').click()\n",
    "    time.sleep(10) \n",
    "    for z in range(1,11):                     \n",
    "        titles = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[1]/span').text  #標題\n",
    "        publication_dates =  driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[2]').text   #日期  \n",
    "        abstracts = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]/div/div[3]').text    #文章摘要 \n",
    "        elements = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[2]/div/div[2]/div/div[2]/div/div/div['+str(z)+']/a[2]')\n",
    "        urls = elements.get_attribute('href')   #連結\n",
    "\n",
    "        driver.get(urls)           \n",
    "        a = driver.find_elements(By.XPATH, '/html/body/div[1]/section[4]/section/div/section/div/div/div[1]/div/div[3]/div[5]')\n",
    "        time.sleep(2)\n",
    "        for item in a:\n",
    "            content = item.text   #內文  \n",
    "        cur.execute('''INSERT INTO videos(title,date,abstract,contents,url) VALUES(?,?,?,?,?);''',(titles,publication_dates,abstracts,content,urls))\n",
    "        conn.commit()\n",
    "        time.sleep(1)\n",
    "\n",
    "        driver.back()\n",
    "        time.sleep(1)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
